{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1dcba0",
   "metadata": {},
   "source": [
    "### Incrementales "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202f55f",
   "metadata": {},
   "source": [
    "### Generar sesion de Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b5e46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No hab√≠a sesi√≥n previa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 16:03:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark version: 2.4.5\n",
      "‚úÖ Master URL: spark://spark-master:7077\n",
      "‚úÖ Spark reiniciado con driver JDBC\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cerrar sesi√≥n actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"‚úÖ Sesi√≥n anterior cerrada\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No hab√≠a sesi√≥n previa\")\n",
    "\n",
    "# Reiniciar con configuraci√≥n expl√≠cita del JAR\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-incremental\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"1\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"1g\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"‚úÖ Spark version:\", spark.version)\n",
    "print(\"‚úÖ Master URL:\", spark.sparkContext.master)\n",
    "print(\"‚úÖ Spark reiniciado con driver JDBC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39d4f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c7b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Conexi√≥n JDBC exitosa!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|test_col|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importar configuraci√≥n\n",
    "import sys\n",
    "sys.path.append(\"/scripts/config\")\n",
    "from db_config import db_config\n",
    "\n",
    "# Probar conexi√≥n\n",
    "try:\n",
    "    df_test = (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", \"SELECT 1 AS test_col\")\n",
    "        .load())\n",
    "    \n",
    "    print(\"‚úÖ ¬°Conexi√≥n JDBC exitosa!\")\n",
    "    df_test.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae8fa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde33eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Tablas disponibles en la base de datos 'olva':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|esquema|nombre_tabla|tipo_tabla|\n",
      "+-------+------------+----------+\n",
      "|dbo    |Clientes    |BASE TABLE|\n",
      "+-------+------------+----------+\n",
      "\n",
      "\n",
      "üìà Total de tablas mostradas: 1\n"
     ]
    }
   ],
   "source": [
    "# Consultar las tablas disponibles (versi√≥n simplificada)\n",
    "try:\n",
    "    df_tablas = (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", \"\"\"\n",
    "            SELECT TOP 100\n",
    "                TABLE_SCHEMA as esquema,\n",
    "                TABLE_NAME as nombre_tabla,\n",
    "                TABLE_TYPE as tipo_tabla\n",
    "            FROM INFORMATION_SCHEMA.TABLES \n",
    "            WHERE TABLE_TYPE = 'BASE TABLE'\n",
    "        \"\"\")\n",
    "        .load())\n",
    "    \n",
    "    print(\"üìä Tablas disponibles en la base de datos 'olva':\")\n",
    "    df_tablas.show(100, truncate=False)\n",
    "    print(f\"\\nüìà Total de tablas mostradas: {df_tablas.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error consultando tablas: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f18a1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_query(query: str):\n",
    "    \"\"\"\n",
    "    Ejecuta un query en SQL Server usando la configuraci√≥n JDBC del proyecto.\n",
    "    Retorna un DataFrame de Spark.\n",
    "    \"\"\"\n",
    "    return (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"dbtable\", query)   # üëà usar dbtable, no query\n",
    "        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccfbc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|      last_watermark|   tabla|\n",
      "+--------------------+--------+\n",
      "|2025-09-10 22:19:...|clientes|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ware_df = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes_watermark\")\n",
    "ware_df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24780498",
   "metadata": {},
   "outputs": [],
   "source": [
    "marca_df= (ware_df\n",
    "           .filter(ware_df.tabla==\"clientes\")\n",
    "           .collect()[0][\"last_watermark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1950cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nuevos registros cargados en Bronze.\n"
     ]
    }
   ],
   "source": [
    "# Construir query incremental con alias obligatorio\n",
    "query_clientes_incr = f\"(SELECT * FROM dbo.Clientes WHERE CreateTime > '{marca_df}') as clientes_incr\"\n",
    "\n",
    "# Leer con funci√≥n corregida\n",
    "df_incremental = read_sql_query(query_clientes_incr)\n",
    "\n",
    "# 3. Verificar si hay datos\n",
    "if df_incremental.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è No hay registros nuevos despu√©s de\", marca_df)\n",
    "    spark.stop()  # Opcional: si este notebook solo carga Clientes, puedes cerrar sesi√≥n\n",
    "    sys.exit(\"üîö Pipeline finalizado: no hab√≠a registros nuevos.\")\n",
    "else:\n",
    "    # 4. Guardar los nuevos registros en Bronze\n",
    "    df_incremental.write.mode(\"append\").parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "    print(\"‚úÖ Nuevos registros cargados en Bronze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7f1565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+----------+--------------------+--------------------+\n",
      "|ClienteID|        Nombre|               Email|  Telefono|          CreateTime|          UpdateTime|\n",
      "+---------+--------------+--------------------+----------+--------------------+--------------------+\n",
      "|        1|    Juan P√©rez|juan.perez@email.com| 089000100|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        2|   Mar√≠a L√≥pez|maria22.nueva@ema...|0987654321|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        3|   Carlos Ruiz|carlos.ruiz@email...|0971122334|2025-08-24 21:27:...|                null|\n",
      "|        4|    Ana Torres|ana.torres@email.com|0965544332|2025-08-24 21:27:...|                null|\n",
      "|        5|   Pedro G√≥mez|pedro.gomez@email...|0956677889|2025-08-24 21:27:...|                null|\n",
      "|        7|     leo P√©rez| leo.perez@email.com|0991234522|2025-08-29 19:50:...|                null|\n",
      "|       12|          dary|  dary.pez@email.com|0991234567|2025-09-10 22:19:...|                null|\n",
      "|       10|       san Per|juan11.pez@email.com|0991234567|2025-09-10 21:51:...|                null|\n",
      "|       11|   san maria L|ma222.lopez@email...|0987654321|2025-09-10 21:51:...|                null|\n",
      "|       13|  leodan toala|juan.toala@email.com|0991234567|2025-09-11 16:02:...|                null|\n",
      "|       14|fabricio total|maria.totala@emai...|0987654321|2025-09-11 16:03:...|                null|\n",
      "|       15|   Carlos Ruiz|carlos.toala@emai...|0971122334|2025-09-11 16:03:...|                null|\n",
      "|        7|     leo P√©rez| leo.perez@email.com|0991234522|2025-08-29 19:50:...|                null|\n",
      "|        8|      Juan Per|  juan.pez@email.com|0991234567|2025-09-10 19:41:...|                null|\n",
      "|        9|       Maria L|  ma.lopez@email.com|0987654321|2025-09-10 19:41:...|                null|\n",
      "+---------+--------------+--------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clie = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "df_clie.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87de007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as spark_max, lit\n",
    "\n",
    "marca_df = df_clie.agg(spark_max(\"CreateTime\").alias(\"last_watermark\")) \\\n",
    "                  .withColumn(\"tabla\", lit(\"clientes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802373c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------+\n",
      "|last_watermark         |tabla   |\n",
      "+-----------------------+--------+\n",
      "|2025-09-11 16:03:17.118|clientes|\n",
      "+-----------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marca_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef4ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "marca_df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/clientes_watermark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c099e8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------+\n",
      "|last_watermark         |tabla   |\n",
      "+-----------------------+--------+\n",
      "|2025-09-11 16:03:17.118|clientes|\n",
      "+-----------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ware_df = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes_watermark\")\n",
    "ware_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf19b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

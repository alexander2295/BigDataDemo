{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60893096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hola Spark desde VS Code 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts/config\")\n",
    "from db_config import db_config\n",
    "\n",
    "print(\"👉 JDBC URL:\", db_config[\"jdbc_url\"])\n",
    "print(\"👉 Usuario:\", db_config[\"user\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20700c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Diagnóstico rápido de entorno/red/Spark =====\n",
    "import os, sys, socket, platform\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"SO:\", platform.platform())\n",
    "print(\"HOSTNAME:\", socket.gethostname())\n",
    "print(\"SPARK_HOME:\", os.environ.get(\"SPARK_HOME\"))\n",
    "\n",
    "# ¿Está pyspark instalado y qué versión?\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"pyspark.__version__:\", pyspark.__version__)\n",
    "except Exception as e:\n",
    "    print(\"pyspark import ERROR:\", e)\n",
    "\n",
    "# ¿Resuelve y conecta a spark-master:7077 y namenode:8020?\n",
    "for host, port in [(\"spark-master\", 7077), (\"namenode\", 8020)]:\n",
    "    try:\n",
    "        s = socket.create_connection((host, port), timeout=3)\n",
    "        print(f\"OK conexión a {host}:{port}\")\n",
    "        s.close()\n",
    "    except Exception as e:\n",
    "        print(f\"FALLO conexión a {host}:{port} → {e.__class__.__name__}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deac080",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.getConf().getAll())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b571421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Spark anterior detenido\")\n",
    "except:\n",
    "    print(\"No había sesión activa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-Spark-HDFS-Dary\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"✅ Spark version:\", spark.version)\n",
    "print(\"✅ Master URL:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ee273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-Spark-HDFS-Dary\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    # 🔑 Forzar a Spark a usar los XML de Hadoop\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:8020\")\n",
    "    .getOrCreate())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c425f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set hive.metastore.uris\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# DataFrame de prueba\n",
    "df = spark.createDataFrame([\n",
    "    Row(id=1, ciudad=\"Manta\", total=185),\n",
    "    Row(id=2, ciudad=\"Lima\",  total=3.0),\n",
    "    Row(id=3, ciudad=\"Quito\", total=4.0),\n",
    "])\n",
    "\n",
    "# Ruta en HDFS\n",
    "out_bronze = \"hdfs://namenode:8020/bronze/data_df/\"\n",
    "\n",
    "# Escritura en HDFS\n",
    "(df.coalesce(1)\n",
    "   .write.mode(\"overwrite\")\n",
    "   .parquet(out_bronze))\n",
    "\n",
    "print(\"✅ Escrito en HDFS →\", out_bronze)\n",
    "\n",
    "# Lectura desde HDFS\n",
    "df2 = spark.read.parquet(out_bronze)\n",
    "print(\"Filas leídas:\", df2.count())\n",
    "df2.orderBy(col(\"total\").desc()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e7b55ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Unable to infer schema for Parquet. It must be specified manually.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o456.parquet.\n: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:185)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:185)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:184)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_859/3949359919.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Leer dataset clientes desde HDFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclientes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://namenode:8020/bronze/clientes/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Unable to infer schema for Parquet. It must be specified manually.;'"
     ]
    }
   ],
   "source": [
    "# Leer dataset clientes desde HDFS\n",
    "clientes = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes/\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5822b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar primeras filas\n",
    "clientes.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85530647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar esquema\n",
    "clientes.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, col\n",
    "\n",
    "# Leer Bronze\n",
    "clientes = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes/\")\n",
    "\n",
    "# Agregar columnas de partición (a partir de CreateTime)\n",
    "clientes_part = (clientes\n",
    "    .withColumn(\"year\", year(col(\"CreateTime\")))\n",
    "    .withColumn(\"month\", month(col(\"CreateTime\")))\n",
    "    .withColumn(\"day\", dayofmonth(col(\"CreateTime\")))\n",
    ")\n",
    "\n",
    "# Guardar en Silver particionado\n",
    "out_silver = \"hdfs://namenode:8020/silver/clientes/\"\n",
    "(clientes_part\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .parquet(out_silver))\n",
    "\n",
    "print(\"✅ Clientes guardados en Silver particionados por CreateTime →\", out_silver)\n",
    "\n",
    "# Validar\n",
    "spark.read.parquet(out_silver).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Verificar si el archivo .env existe\n",
    "env_path = \"/etc/credenciales/.env\"\n",
    "print(f\"📁 Verificando archivo: {env_path}\")\n",
    "print(f\"📁 Archivo existe: {os.path.exists(env_path)}\")\n",
    "\n",
    "# Forzar recarga del archivo .env\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# Debug: Ver todas las variables de entorno que empiecen con DB_\n",
    "print(\"\\n🔍 Variables de entorno DB_:\")\n",
    "for key, value in os.environ.items():\n",
    "    if key.startswith(\"DB_\"):\n",
    "        print(f\"  {key} = {value}\")\n",
    "\n",
    "# Configuración de la base de datos\n",
    "db_config = {\n",
    "    \"jdbc_url\": os.getenv(\"DB_JDBC_URL\"),\n",
    "    \"database\": os.getenv(\"DB_DATABASE\"),\n",
    "    \"driver\": os.getenv(\"DB_DRIVER\"),\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASS\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\")\n",
    "}\n",
    "\n",
    "print(\"\\n✅ Config cargada:\", db_config)\n",
    "\n",
    "# Debug adicional: verificar cada variable individualmente\n",
    "print(\"\\n🔎 Debug individual:\")\n",
    "variables = [\"DB_JDBC_URL\", \"DB_DATABASE\", \"DB_DRIVER\", \"DB_USER\", \"DB_PASS\", \"DB_PORT\"]\n",
    "for var in variables:\n",
    "    value = os.getenv(var)\n",
    "    print(f\"  {var}: {value} ({'✅ OK' if value else '❌ MISSING'})\")\n",
    "\n",
    "# Leer archivo .env directamente para debug\n",
    "try:\n",
    "    print(f\"\\n📖 Contenido del archivo {env_path}:\")\n",
    "    with open(env_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            # Mostrar caracteres especiales\n",
    "            line_repr = repr(line.rstrip())\n",
    "            print(f\"  Línea {i}: {line_repr}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error leyendo archivo: {e}\")\n",
    "\n",
    "# Función para cargar manualmente si dotenv falla\n",
    "def load_env_manual(file_path):\n",
    "    print(f\"\\n🔧 Intentando carga manual del archivo...\")\n",
    "    env_vars = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                original_line = line\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '=' in line:\n",
    "                    try:\n",
    "                        key, value = line.split('=', 1)\n",
    "                        key = key.strip()\n",
    "                        value = value.strip()\n",
    "                        env_vars[key] = value\n",
    "                        os.environ[key] = value\n",
    "                        print(f\"  ✅ Cargada: {key} = {value}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"  ❌ Error en línea {line_num}: {repr(original_line.strip())} -> {e}\")\n",
    "                elif line:\n",
    "                    print(f\"  ⚠️  Línea ignorada {line_num}: {repr(original_line.strip())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en carga manual: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    return env_vars\n",
    "\n",
    "# Intentar carga manual para las variables faltantes\n",
    "print(\"\\n⚠️  Algunas variables faltan, intentando carga manual...\")\n",
    "manual_vars = load_env_manual(env_path)\n",
    "\n",
    "# Verificar de nuevo después de la carga manual\n",
    "print(\"\\n🔄 Verificación después de carga manual:\")\n",
    "for var in variables:\n",
    "    value = os.getenv(var)\n",
    "    print(f\"  {var}: {value} ({'✅ OK' if value else '❌ STILL MISSING'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ab181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-SQLServer\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")  # 👈 necesario\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152499ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", db_config[\"jdbc_url\"])\n",
    "    .option(\"dbtable\", \"dbo.Clientes\")   # 👈 ajusta a tu tabla real\n",
    "    .option(\"user\", db_config[\"user\"])\n",
    "    .option(\"password\", db_config[\"password\"])\n",
    "    .option(\"driver\", db_config[\"driver\"])\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc299b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts/config\")   # ruta donde vive db_config.py\n",
    "from db_config import db_config\n",
    "\n",
    "print(db_config[\"jdbc_url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Contenido completo de db_config ===\")\n",
    "for key, value in db_config.items():\n",
    "    if key == \"password\":\n",
    "        print(f\"{key}: {'*' * len(str(value))}\")  # Ocultar password\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jar_path = \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\"\n",
    "print(f\"JAR exists: {os.path.exists(jar_path)}\")\n",
    "\n",
    "if os.path.exists(jar_path):\n",
    "    print(f\"JAR size: {os.path.getsize(jar_path)} bytes\")\n",
    "    print(\"✅ El JAR está montado correctamente\")\n",
    "else:\n",
    "    print(\"❌ El JAR no está disponible en el contenedor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70342d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ No había sesión previa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 16:07:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark version: 2.4.5\n",
      "✅ Master URL: spark://spark-master:7077\n",
      "✅ Spark reiniciado con driver JDBC\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cerrar sesión actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✅ Sesión anterior cerrada\")\n",
    "except:\n",
    "    print(\"ℹ️ No había sesión previa\")\n",
    "\n",
    "# Reiniciar con configuración explícita del JAR\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-Spark-HDFS-Dary\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"✅ Spark version:\", spark.version)\n",
    "print(\"✅ Master URL:\", spark.sparkContext.master)\n",
    "print(\"✅ Spark reiniciado con driver JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jar_path = \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\"\n",
    "print(f\"JAR exists: {os.path.exists(jar_path)}\")\n",
    "\n",
    "if os.path.exists(jar_path):\n",
    "    print(f\"JAR size: {os.path.getsize(jar_path)} bytes\")\n",
    "    print(\"✅ El JAR está disponible!\")\n",
    "else:\n",
    "    print(\"❌ El JAR aún no está disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29e9207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ¡Conexión JDBC exitosa!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|test_col|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importar configuración\n",
    "import sys\n",
    "sys.path.append(\"/scripts/config\")\n",
    "from db_config import db_config\n",
    "\n",
    "# Probar conexión\n",
    "try:\n",
    "    df_test = (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", \"SELECT 1 AS test_col\")\n",
    "        .load())\n",
    "    \n",
    "    print(\"✅ ¡Conexión JDBC exitosa!\")\n",
    "    df_test.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d49e4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Tablas disponibles en la base de datos 'olva':\n",
      "+-------+------------+----------+\n",
      "|esquema|nombre_tabla|tipo_tabla|\n",
      "+-------+------------+----------+\n",
      "|dbo    |Clientes    |BASE TABLE|\n",
      "+-------+------------+----------+\n",
      "\n",
      "\n",
      "📈 Total de tablas mostradas: 1\n"
     ]
    }
   ],
   "source": [
    "# Consultar las tablas disponibles (versión simplificada)\n",
    "try:\n",
    "    df_tablas = (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", \"\"\"\n",
    "            SELECT TOP 100\n",
    "                TABLE_SCHEMA as esquema,\n",
    "                TABLE_NAME as nombre_tabla,\n",
    "                TABLE_TYPE as tipo_tabla\n",
    "            FROM INFORMATION_SCHEMA.TABLES \n",
    "            WHERE TABLE_TYPE = 'BASE TABLE'\n",
    "        \"\"\")\n",
    "        .load())\n",
    "    \n",
    "    print(\"📊 Tablas disponibles en la base de datos 'olva':\")\n",
    "    df_tablas.show(100, truncate=False)\n",
    "    print(f\"\\n📈 Total de tablas mostradas: {df_tablas.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error consultando tablas: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb329041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_query(query: str):\n",
    "    \"\"\"\n",
    "    Ejecuta un query en SQL Server usando la configuración JDBC del proyecto.\n",
    "    Retorna un DataFrame de Spark.\n",
    "    \"\"\"\n",
    "    return (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", query)\n",
    "        .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6d70761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes = read_sql_query(\"SELECT TOP 10 * FROM dbo.Clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "833c5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|ClienteID|     Nombre|               Email|  Telefono|          CreateTime|          UpdateTime|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|        1| Juan Pérez|juan.perez@email.com| 089000100|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        2|María López|maria22.nueva@ema...|0987654321|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        3|Carlos Ruiz|carlos.ruiz@email...|0971122334|2025-08-24 21:27:...|                null|\n",
      "|        4| Ana Torres|ana.torres@email.com|0965544332|2025-08-24 21:27:...|                null|\n",
      "|        5|Pedro Gómez|pedro.gomez@email...|0956677889|2025-08-24 21:27:...|                null|\n",
      "|        7|  leo Pérez| leo.perez@email.com|0991234522|2025-08-29 19:50:...|                null|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clientes.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5bdf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/clientes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7caaf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clientes.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/cliente_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b36c45dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|ClienteID|     Nombre|               Email|  Telefono|          CreateTime|          UpdateTime|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|        1| Juan Pérez|juan.perez@email.com| 089000100|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        2|María López|maria22.nueva@ema...|0987654321|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        3|Carlos Ruiz|carlos.ruiz@email...|0971122334|2025-08-24 21:27:...|                null|\n",
      "|        4| Ana Torres|ana.torres@email.com|0965544332|2025-08-24 21:27:...|                null|\n",
      "|        5|Pedro Gómez|pedro.gomez@email...|0956677889|2025-08-24 21:27:...|                null|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_validacion = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "df_validacion.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6dcf7b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|ClienteID|Nombre     |Email                  |Telefono  |CreateTime             |UpdateTime             |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|1        |Juan Pérez |juan.perez@email.com   |089000100 |2025-08-24 21:27:20.383|2025-08-24 21:31:48.536|\n",
      "|2        |María López|maria22.nueva@email.com|0987654321|2025-08-24 21:27:20.383|2025-08-24 21:31:59.697|\n",
      "|3        |Carlos Ruiz|carlos.ruiz@email.com  |0971122334|2025-08-24 21:27:20.383|null                   |\n",
      "|4        |Ana Torres |ana.torres@email.com   |0965544332|2025-08-24 21:27:20.383|null                   |\n",
      "|5        |Pedro Gómez|pedro.gomez@email.com  |0956677889|2025-08-24 21:27:20.383|null                   |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as spark_max\n",
    "df_clientes = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "df_clientes.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6efae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  ultima_fecha_carga|\n",
      "+--------------------+\n",
      "|2025-08-29 19:50:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clientes.agg(spark_max(\"CreateTime\").alias(\"ultima_fecha_carga\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb1c6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# Crear DF con el watermark\n",
    "watermark_df = spark.createDataFrame([Row(tabla=\"clientes\",last_watermark=\"2025-08-29 19:50:42\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8acbd0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Watermark inicial guardado en /control/clientes_watermark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Guardar en HDFS\n",
    "watermark_df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/clientes_watermark\")\n",
    "\n",
    "print(\"✅ Watermark inicial guardado en /control/clientes_watermark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e20a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|     last_watermark|   tabla|\n",
      "+-------------------+--------+\n",
      "|2025-08-29 19:50:42|clientes|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ware_df = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes_watermark\")\n",
    "ware_df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cd95256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultima carga de fecha de cliente 2025-08-29 19:50:42\n"
     ]
    }
   ],
   "source": [
    "marca_df= (ware_df\n",
    "           .filter(ware_df.tabla==\"clientes\")\n",
    "           .collect()[0][\"last_watermark\"])\n",
    "print(\"ultima carga de fecha de cliente\",marca_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b115b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_query(query: str):\n",
    "    \"\"\"\n",
    "    Ejecuta un query en SQL Server usando la configuración JDBC del proyecto.\n",
    "    Retorna un DataFrame de Spark.\n",
    "    \"\"\"\n",
    "    return (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"dbtable\", query)   # 👈 usar dbtable, no query\n",
    "        .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4c64aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultima carga de fecha de cliente 2025-08-29 19:50:42\n"
     ]
    }
   ],
   "source": [
    "marca_df= (ware_df\n",
    "           .filter(ware_df.tabla==\"clientes\")\n",
    "           .collect()[0][\"last_watermark\"])\n",
    "print(\"ultima carga de fecha de cliente\",marca_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b29e06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+----------+-----------------------+----------+\n",
      "|ClienteID|Nombre   |Email              |Telefono  |CreateTime             |UpdateTime|\n",
      "+---------+---------+-------------------+----------+-----------------------+----------+\n",
      "|7        |leo Pérez|leo.perez@email.com|0991234522|2025-08-29 19:50:42.572|null      |\n",
      "|8        |Juan Per |juan.pez@email.com |0991234567|2025-09-10 19:41:11.791|null      |\n",
      "|9        |Maria L  |ma.lopez@email.com |0987654321|2025-09-10 19:41:11.791|null      |\n",
      "+---------+---------+-------------------+----------+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construir query incremental con alias obligatorio\n",
    "query_clientes_incr = f\"(SELECT * FROM dbo.Clientes WHERE CreateTime > '{marca_df}') as clientes_incr\"\n",
    "\n",
    "# Leer con función corregida\n",
    "df_incremental = read_sql_query(query_clientes_incr)\n",
    "df_incremental.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49fc8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos incrementales guardados en Bronze/clientes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_incremental.write.mode(\"append\").parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "print(\"✅ Datos incrementales guardados en Bronze/clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "324600a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|ClienteID|Nombre     |Email                  |Telefono  |CreateTime             |UpdateTime             |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|1        |Juan Pérez |juan.perez@email.com   |089000100 |2025-08-24 21:27:20.383|2025-08-24 21:31:48.536|\n",
      "|2        |María López|maria22.nueva@email.com|0987654321|2025-08-24 21:27:20.383|2025-08-24 21:31:59.697|\n",
      "|3        |Carlos Ruiz|carlos.ruiz@email.com  |0971122334|2025-08-24 21:27:20.383|null                   |\n",
      "|4        |Ana Torres |ana.torres@email.com   |0965544332|2025-08-24 21:27:20.383|null                   |\n",
      "|5        |Pedro Gómez|pedro.gomez@email.com  |0956677889|2025-08-24 21:27:20.383|null                   |\n",
      "|7        |leo Pérez  |leo.perez@email.com    |0991234522|2025-08-29 19:50:42.572|null                   |\n",
      "|7        |leo Pérez  |leo.perez@email.com    |0991234522|2025-08-29 19:50:42.572|null                   |\n",
      "|8        |Juan Per   |juan.pez@email.com     |0991234567|2025-09-10 19:41:11.791|null                   |\n",
      "|9        |Maria L    |ma.lopez@email.com     |0987654321|2025-09-10 19:41:11.791|null                   |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clie = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "df_clie.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec83587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  ultima_fecha_carga|\n",
      "+--------------------+\n",
      "|2025-09-10 19:41:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clie.agg(spark_max(\"CreateTime\").alias(\"ultima_fecha_carga\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f814f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b905daaa",
   "metadata": {},
   "source": [
    "### actualizar clientes ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7216bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No hab√≠a sesi√≥n previa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 20:21:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark version: 2.4.5\n",
      "‚úÖ Master URL: spark://spark-master:7077\n",
      "‚úÖ Spark reiniciado con driver JDBC\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cerrar sesi√≥n actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"‚úÖ Sesi√≥n anterior cerrada\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No hab√≠a sesi√≥n previa\")\n",
    "\n",
    "# Reiniciar con configuraci√≥n expl√≠cita del JAR\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-update-clientes\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"1\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"1g\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"‚úÖ Spark version:\", spark.version)\n",
    "print(\"‚úÖ Master URL:\", spark.sparkContext.master)\n",
    "print(\"‚úÖ Spark reiniciado con driver JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import Row\n",
    "# Crear DF con el watermark\n",
    "#watermark_df = spark.createDataFrame([Row(tabla=\"update_clientes\",last_watermark=\"2025-08-24 21:31:59.697\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#watermark_df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/update_clientes_watermark\")\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Dataset creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa9ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_watermark= spark.read.parquet(\"hdfs://namenode:8020/bronze/update_clientes_watermark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c4dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|      last_watermark|          tabla|\n",
      "+--------------------+---------------+\n",
      "|2025-08-24 21:31:...|update_clientes|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_watermark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a5a2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïí √öltima marca de agua usada: 2025-08-24 21:31:59.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "marca_df = df_watermark.filter(df_watermark.tabla == \"update_clientes\") \\\n",
    "                       .select(\"last_watermark\") \\\n",
    "                       .collect()[0][0]\n",
    "\n",
    "print(\"üïí √öltima marca de agua usada:\", marca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae43d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------------------------+----------+-----------------------+-----------------------+\n",
      "|ClienteID|Nombre        |Email                       |Telefono  |CreateTime             |UpdateTime             |\n",
      "+---------+--------------+----------------------------+----------+-----------------------+-----------------------+\n",
      "|2        |Mar√≠a L√≥pez   |mar23928.actualice@email.com|0987654321|2025-08-24 21:27:20.383|2025-09-11 17:22:59.786|\n",
      "|14       |fabricio total|fabrico.update@email.com    |0987654321|2025-09-11 16:03:07.859|2025-09-11 20:32:42.701|\n",
      "+---------+--------------+----------------------------+----------+-----------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query_clientes_incr = f\"\"\"\n",
    "(SELECT *\n",
    " FROM dbo.Clientes\n",
    " WHERE UpdateTime > '{marca_df}') as clientes_incr\n",
    "\"\"\"\n",
    "\n",
    "df_updates = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:sqlserver://host.docker.internal:1433;databaseName=olva;encrypt=false\")\n",
    "    .option(\"dbtable\", query_clientes_incr)\n",
    "    .option(\"user\", \"etl_user\")\n",
    "    .option(\"password\", \"StrongPass123\")\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "    .load())\n",
    "\n",
    "df_updates.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee539fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos reorganizados con particiones por CreateTime\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Leer clientes sin partici√≥n\n",
    "#df_clientes = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "\n",
    "# Crear columna de partici√≥n a partir de CreateTime\n",
    "#df_clientes = df_clientes.withColumn(\"fecha\", to_date(col(\"CreateTime\")))\n",
    "\n",
    "# Reescribir particionado por fecha (de creaci√≥n)\n",
    "#df_clientes.write.mode(\"overwrite\") \\\n",
    " #   .partitionBy(\"fecha\") \\\n",
    " #   .parquet(\"hdfs://namenode:8020/bronze/clientes_partitioned\")\n",
    "\n",
    "#print(\"‚úÖ Datos reorganizados con particiones por CreateTime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e1f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "BRONZE_PATH = \"hdfs://namenode:8020/bronze/clientes_partitioned\"\n",
    "WATERMARK_PATH = \"hdfs://namenode:8020/bronze/update_clientes_watermark\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43314ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agrega la columna de partici√≥n (fecha) a partir de CreateTime\n",
    "df_updates = df_updates.withColumn(\"fecha\", F.to_date(F.col(\"CreateTime\")))\n",
    "\n",
    "# Opcional: cache si el lote es mediano/grande\n",
    "df_updates.cache()\n",
    "df_updates.count()  # fuerza materializaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ec6a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>  (190 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Particiones afectadas: [datetime.date(2025, 9, 11), datetime.date(2025, 8, 24)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "particiones_afectadas = [r[\"fecha\"] for r in df_updates.select(\"fecha\").distinct().collect()]\n",
    "print(\"üìå Particiones afectadas:\", particiones_afectadas)\n",
    "\n",
    "if not particiones_afectadas:\n",
    "    print(\"‚úÖ No hay filas nuevas/actualizadas. Nada que sobrescribir.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f223fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existente = (spark.read.parquet(BRONZE_PATH)\n",
    "                .filter(F.col(\"fecha\").isin(particiones_afectadas)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedb2e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     fecha|\n",
      "+----------+\n",
      "|2025-09-11|\n",
      "|2025-08-24|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reordenar columnas de df_updates al mismo orden de df_existente\n",
    "df_updates = df_updates.select(df_existente.columns)\n",
    "\n",
    "# Uni√≥n segura\n",
    "df_union = df_existente.unionByName(df_updates)\n",
    "\n",
    "# √öltima versi√≥n por (fecha, ClienteID) usando UpdateTime\n",
    "w = Window.partitionBy(\"fecha\", \"ClienteID\").orderBy(F.col(\"UpdateTime\").desc())\n",
    "\n",
    "df_final = (df_union\n",
    "            .withColumn(\"rn\", F.row_number().over(w))\n",
    "            .filter(F.col(\"rn\") == 1)\n",
    "            .drop(\"rn\"))\n",
    "\n",
    "df_final.count(), df_final.select(\"fecha\").distinct().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7180dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:================================================>     (181 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Particiones sobrescritas din√°micamente en Bronze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df_final.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\")\n",
    "    .partitionBy(\"fecha\")\n",
    "    .parquet(BRONZE_PATH))\n",
    "\n",
    "print(\"‚úÖ Particiones sobrescritas din√°micamente en Bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b396b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Vista partici√≥n fecha=2025-08-24:\n",
      "+---------+-----------+----------------------------+----------+-----------------------+-----------------------+----------+\n",
      "|ClienteID|Nombre     |Email                       |Telefono  |CreateTime             |UpdateTime             |fecha     |\n",
      "+---------+-----------+----------------------------+----------+-----------------------+-----------------------+----------+\n",
      "|1        |Juan P√©rez |juan.perez@email.com        |089000100 |2025-08-24 21:27:20.383|2025-08-24 21:31:48.536|2025-08-24|\n",
      "|2        |Mar√≠a L√≥pez|mar23928.actualice@email.com|0987654321|2025-08-24 21:27:20.383|2025-09-11 17:22:59.786|2025-08-24|\n",
      "|3        |Carlos Ruiz|carlos.ruiz@email.com       |0971122334|2025-08-24 21:27:20.383|null                   |2025-08-24|\n",
      "|4        |Ana Torres |ana.torres@email.com        |0965544332|2025-08-24 21:27:20.383|null                   |2025-08-24|\n",
      "|5        |Pedro G√≥mez|pedro.gomez@email.com       |0956677889|2025-08-24 21:27:20.383|null                   |2025-08-24|\n",
      "+---------+-----------+----------------------------+----------+-----------------------+-----------------------+----------+\n",
      "\n",
      "üîé Vista partici√≥n fecha=2025-09-11:\n",
      "+---------+--------------+------------------------+----------+-----------------------+-----------------------+----------+\n",
      "|ClienteID|Nombre        |Email                   |Telefono  |CreateTime             |UpdateTime             |fecha     |\n",
      "+---------+--------------+------------------------+----------+-----------------------+-----------------------+----------+\n",
      "|13       |leodan toala  |juan.toala@email.com    |0991234567|2025-09-11 16:02:57.701|null                   |2025-09-11|\n",
      "|14       |fabricio total|fabrico.update@email.com|0987654321|2025-09-11 16:03:07.859|2025-09-11 20:32:42.701|2025-09-11|\n",
      "|15       |Carlos Ruiz   |carlos.toala@email.com  |0971122334|2025-09-11 16:03:17.118|null                   |2025-09-11|\n",
      "+---------+--------------+------------------------+----------+-----------------------+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in [\"2025-08-24\", \"2025-09-11\"]:\n",
    "    print(f\"üîé Vista partici√≥n fecha={f}:\")\n",
    "    (spark.read.parquet(BRONZE_PATH)\n",
    "          .filter(F.col(\"fecha\") == f)\n",
    "          .orderBy(\"ClienteID\")\n",
    "          .show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5fc0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+----------+--------------------+--------------------+----------+\n",
      "|ClienteID|        Nombre|               Email|  Telefono|          CreateTime|          UpdateTime|     fecha|\n",
      "+---------+--------------+--------------------+----------+--------------------+--------------------+----------+\n",
      "|        2|   Mar√≠a L√≥pez|mar23928.actualic...|0987654321|2025-08-24 21:27:...|2025-09-11 17:22:...|2025-08-24|\n",
      "|       14|fabricio total|fabrico.update@em...|0987654321|2025-09-11 16:03:...|2025-09-11 20:32:...|2025-09-11|\n",
      "|        1|    Juan P√©rez|juan.perez@email.com| 089000100|2025-08-24 21:27:...|2025-08-24 21:31:...|2025-08-24|\n",
      "|       15|   Carlos Ruiz|carlos.toala@emai...|0971122334|2025-09-11 16:03:...|                null|2025-09-11|\n",
      "|        5|   Pedro G√≥mez|pedro.gomez@email...|0956677889|2025-08-24 21:27:...|                null|2025-08-24|\n",
      "|       13|  leodan toala|juan.toala@email.com|0991234567|2025-09-11 16:02:...|                null|2025-09-11|\n",
      "|        3|   Carlos Ruiz|carlos.ruiz@email...|0971122334|2025-08-24 21:27:...|                null|2025-08-24|\n",
      "|        4|    Ana Torres|ana.torres@email.com|0965544332|2025-08-24 21:27:...|                null|2025-08-24|\n",
      "|        7|     leo P√©rez| leo.perez@email.com|0991234522|2025-08-29 19:50:...|                null|2025-08-29|\n",
      "|        7|     leo P√©rez| leo.perez@email.com|0991234522|2025-08-29 19:50:...|                null|2025-08-29|\n",
      "|        8|      Juan Per|  juan.pez@email.com|0991234567|2025-09-10 19:41:...|                null|2025-09-10|\n",
      "|        9|       Maria L|  ma.lopez@email.com|0987654321|2025-09-10 19:41:...|                null|2025-09-10|\n",
      "|       12|          dary|  dary.pez@email.com|0991234567|2025-09-10 22:19:...|                null|2025-09-10|\n",
      "|       10|       san Per|juan11.pez@email.com|0991234567|2025-09-10 21:51:...|                null|2025-09-10|\n",
      "|       11|   san maria L|ma222.lopez@email...|0987654321|2025-09-10 21:51:...|                null|2025-09-10|\n",
      "+---------+--------------+--------------------+----------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bronze = spark.read.parquet(BRONZE_PATH)\n",
    "df_bronze.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488614f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïí Nueva marca de agua: 2025-09-11 20:32:42.701000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "# Calcular el nuevo m√°ximo de UpdateTime en los updates procesados\n",
    "nueva_marca = df_updates.agg(spark_max(\"UpdateTime\")).collect()[0][0]\n",
    "\n",
    "print(\"üïí Nueva marca de agua:\", nueva_marca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c6ce4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df_nueva_marca = spark.createDataFrame([\n",
    "    Row(last_watermark=nueva_marca, tabla=\"update_clientes\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fd67067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nueva_marca.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/update_clientes_watermark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe6c9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"hdfs://namenode:8020/bronze/update_clientes_watermark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6875fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+\n",
      "|last_watermark         |tabla          |\n",
      "+-----------------------+---------------+\n",
      "|2025-09-11 20:32:42.701|update_clientes|\n",
      "+-----------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ec53b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cache de Spark liberado\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.clearCache()\n",
    "print(\"‚úÖ Cache de Spark liberado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff70f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9988a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
